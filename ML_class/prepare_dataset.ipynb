{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from paus_utils import w_central\n",
    "\n",
    "from jpasLAEs.utils import flux_to_mag, bin_centers\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from load_paus_mocks import load_mock_dict\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'W3'\n",
    "savedir = '/home/alberto/almacen/PAUS_data/LF_corrections'\n",
    "\n",
    "nb_min, nb_max = 0, 16\n",
    "\n",
    "with open(f'{savedir}/mock_dict_{field_name}_nb{nb_min}-{nb_max}.pkl', 'rb') as f:\n",
    "    mock_dict = pickle.load(f)\n",
    "\n",
    "mock_dict['SFG'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minimum number of candidates to set the set length\n",
    "N_candidates_list = []\n",
    "for mock_name, mock in mock_dict.items():\n",
    "    N_candidates_list.append(sum(mock['nice_lya']))\n",
    "\n",
    "set_len = np.min(N_candidates_list)\n",
    "print(f'{set_len=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the set for each class\n",
    "tt_set = None\n",
    "labels = None\n",
    "rmag = None\n",
    "zspec = None\n",
    "L_Arr = None\n",
    "\n",
    "for mock_name, mock in mock_dict.items():\n",
    "    mock_len = len(mock['zspec'])\n",
    "    nice_lya = mock['nice_lya']\n",
    "    np.random.seed(299792458)\n",
    "    selection = np.random.choice(np.arange(mock_len)[nice_lya], set_len,\n",
    "                                 replace=False)\n",
    "    this_set = np.hstack([\n",
    "        mock['flx'][:40, selection].T * 1e17,\n",
    "        mock['err'][:40, selection].T / mock['flx'][:40, selection].T,\n",
    "        mock['lya_NB'][selection].reshape(-1, 1),\n",
    "    ])\n",
    "\n",
    "    if tt_set is None:\n",
    "        tt_set = this_set\n",
    "        this_rmag = flux_to_mag(mock['flx'][-4, selection], w_central[-4])\n",
    "        rmag = this_rmag\n",
    "        zspec = mock['zspec'][selection]\n",
    "        L_Arr = mock['L_lya'][selection]\n",
    "    else:\n",
    "        tt_set = np.vstack([tt_set, this_set])\n",
    "\n",
    "        this_rmag = flux_to_mag(mock['flx'][-4, selection], w_central[-4])\n",
    "        rmag = np.concatenate([rmag, this_rmag])\n",
    "        zspec = np.concatenate([zspec, mock['zspec'][selection]])\n",
    "        L_Arr = np.concatenate([L_Arr, mock['L_lya'][selection]])\n",
    "    \n",
    "\n",
    "label_names = []\n",
    "for i in range(len(mock_dict)):\n",
    "    mock_name = list(mock_dict.keys())[i]\n",
    "    print(f'{i} for {mock_name}')\n",
    "    if labels is None:\n",
    "        labels = np.ones(set_len).astype(int) * i\n",
    "    else:\n",
    "        labels = np.concatenate([labels, np.ones(set_len).astype(int) * i])\n",
    "    label_names.append(mock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_seed = 299792458\n",
    "x_train, x_test, y_train, y_test =\\\n",
    "    model_selection.train_test_split(tt_set, labels, test_size=0.2,\n",
    "                                     random_state=split_seed)\n",
    "\n",
    "## Pre-processing ##\n",
    "\n",
    "# PCA\n",
    "# pca = PCA(n_components=0.99, svd_solver='full')\n",
    "\n",
    "# pca.fit(x_train)\n",
    "# x_train = pca.transform(x_train)\n",
    "# x_test = pca.transform(x_test)\n",
    "# print(x_train.shape)\n",
    "\n",
    "# Standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_grid_search():\n",
    "    # Create the parameter grid based on the results of random search\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(60, 60), (60, 40), (30, 30), (20, 20), (40, 20)],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [1e-4, 1e-5, 1e-6],\n",
    "        'batch_size': [300, 500, 750, 'auto'],\n",
    "        'learning_rate': ['adaptive', 'constant'],\n",
    "        'max_iter': [10000],\n",
    "        'n_iter_no_change': [10],\n",
    "        'shuffle': [False, True]\n",
    "    }\n",
    "    # Create a based model\n",
    "    nn = MLPClassifier()\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        estimator=nn, param_distributions=param_grid,\n",
    "        cv=5, n_jobs=-1, pre_dispatch='2*n_jobs',\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "best_params = do_grid_search()\n",
    "\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_best = MLPClassifier(**best_params)\n",
    "cl_best.fit(x_train, y_train)\n",
    "test_score = cl_best.score(x_test, y_test)\n",
    "train_score = cl_best.score(x_train, y_train)\n",
    "print(f'Score\\n\\nTrain: {train_score:0.3f}\\nTest: {test_score:0.3f}')\n",
    "\n",
    "# Predict test\n",
    "pred_test = cl_best.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmag_train, rmag_test =\\\n",
    "    model_selection.train_test_split(rmag, test_size=0.2, random_state=split_seed)\n",
    "zspec_train, zspec_test =\\\n",
    "    model_selection.train_test_split(zspec, test_size=0.2, random_state=split_seed)\n",
    "L_Arr_train, L_Arr_test =\\\n",
    "    model_selection.train_test_split(L_Arr, test_size=0.2, random_state=split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "r_mask = (rmag_test < 22.5)\n",
    "cm = confusion_matrix(y_test[r_mask], pred_test[r_mask])\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.2f',\n",
    "            xticklabels=label_names, yticklabels=label_names,\n",
    "            cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title('r < 22.5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "r_mask = (rmag_test >= 2)\n",
    "cm = confusion_matrix(y_test[r_mask], pred_test[r_mask])\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.2f',\n",
    "            xticklabels=label_names, yticklabels=label_names,\n",
    "            cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title('r $\\geq$ 0')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
