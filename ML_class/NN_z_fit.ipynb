{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from paus_utils import w_central, z_NB\n",
    "\n",
    "from jpasLAEs.utils import flux_to_mag, bin_centers\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from load_paus_mocks import load_mock_dict\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'W3'\n",
    "savedir = '/home/alberto/almacen/PAUS_data/LF_corrections'\n",
    "\n",
    "nb_min, nb_max = 0, 18\n",
    "\n",
    "with open(f'{savedir}/mock_dict_{field_name}_nb{nb_min}-{nb_max}.pkl', 'rb') as f:\n",
    "    mock_dict = pickle.load(f)\n",
    "\n",
    "del mock_dict['SFG']\n",
    "del mock_dict['GAL']\n",
    "# del mock_dict['QSO_cont']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minimum number of candidates to set the set length\n",
    "N_candidates_list = []\n",
    "for mock_name, mock in mock_dict.items():\n",
    "    z_phot = z_NB(mock['lya_NB'])\n",
    "    nice_z = np.abs(mock['zspec'] - z_phot) < 0.12\n",
    "\n",
    "    if mock_name in ['QSO_LAEs_loL', 'QSO_LAEs_hiL', 'QSO_cont']:\n",
    "        N_candidates_list.append(sum(mock['nice_lya_0'][nice_z]))\n",
    "    else:\n",
    "        N_candidates_list.append(sum(mock['nice_lya_0']))\n",
    "\n",
    "set_len = np.min(N_candidates_list)\n",
    "print(N_candidates_list)\n",
    "print(f'{set_len=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the set for each class\n",
    "tt_set = None\n",
    "labels = None\n",
    "rmag = None\n",
    "zspec = None\n",
    "zphot = None\n",
    "L_Arr = None\n",
    "\n",
    "nice_z_list = []\n",
    "\n",
    "labels = None\n",
    "\n",
    "for mock_name, mock in mock_dict.items():\n",
    "    mock_len = len(mock['zspec'])\n",
    "    nice_lya = mock['nice_lya_0']\n",
    "    r_mag = mock['r_mag']\n",
    "\n",
    "    z_phot = z_NB(mock['lya_NB'])\n",
    "    nice_z = np.abs(np.array(mock['zspec']) - z_phot) < 0.12\n",
    "\n",
    "    # np.random.seed(299792458)\n",
    "    # selection = np.random.choice(np.arange(mock_len)[nice_lya], set_len,\n",
    "    #                              replace=False)\n",
    "    selection = np.arange(mock_len)[nice_z & (r_mag > 0)]\n",
    "\n",
    "    this_set = np.hstack([\n",
    "        mock['flx'][:40, selection].T * 1e17, # NBs\n",
    "        mock['r_mag'][selection].reshape(-1, 1),\n",
    "        mock['flx'][40:45, selection].T * 1e17, # BBs\n",
    "        mock['lya_NB'][selection].reshape(-1, 1),\n",
    "    ])\n",
    "\n",
    "    if tt_set is None:\n",
    "        tt_set = this_set\n",
    "        this_rmag = flux_to_mag(mock['flx'][-4, selection], w_central[-4])\n",
    "        rmag = this_rmag\n",
    "        zspec = mock['zspec'][selection]\n",
    "        L_Arr = mock['L_lya'][selection]\n",
    "        zphot = z_NB(mock['lya_NB'])[selection]\n",
    "    else:\n",
    "        tt_set = np.vstack([tt_set, this_set])\n",
    "\n",
    "        this_rmag = flux_to_mag(mock['flx'][-4, selection], w_central[-4])\n",
    "        rmag = np.concatenate([rmag, this_rmag])\n",
    "        zspec = np.concatenate([zspec, mock['zspec'][selection]])\n",
    "        L_Arr = np.concatenate([L_Arr, mock['L_lya'][selection]])\n",
    "        zphot = np.concatenate([zphot, z_NB(mock['lya_NB'])[selection]])\n",
    "\n",
    "    nice_z_list.append(nice_z[selection])\n",
    "    \n",
    "# Labels are z_spec\n",
    "labels = zspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_seed = 299792458\n",
    "x_train, x_test, y_train, y_test =\\\n",
    "    model_selection.train_test_split(tt_set, labels, test_size=0.2,\n",
    "                                     random_state=split_seed)\n",
    "\n",
    "## Pre-processing ##\n",
    "x_train[:, :40] /= np.sum(x_train[:, :40], axis=1).reshape(-1, 1)\n",
    "x_train[:, 41:46] /= np.sum(x_train[:, 41:46], axis=1).reshape(-1, 1)\n",
    "\n",
    "x_test[:, :40] /= np.sum(x_test[:, :40], axis=1).reshape(-1, 1)\n",
    "x_test[:, 41:46] /= np.sum(x_test[:, 41:46], axis=1).reshape(-1, 1)\n",
    "\n",
    "## Scaler\n",
    "# scaler = MinMaxScaler()\n",
    "x_train[:, 40] /= 30\n",
    "x_test[:, 40] /= 30\n",
    "x_train[:, 46] /= 30\n",
    "x_test[:, 46] /= 30\n",
    "# Apply scaling only to fluxes\n",
    "# scaler.fit(x_train[:, :46])\n",
    "# x_train[:, :46] = scaler.transform(x_train[:, :46])\n",
    "# x_test[:, :46] = scaler.transform(x_test[:, :46])\n",
    "\n",
    "# PCA\n",
    "# pca = PCA(n_components=0.99, svd_solver='full')\n",
    "\n",
    "# pca.fit(x_train)\n",
    "# x_train = pca.transform(x_train)\n",
    "# x_test = pca.transform(x_test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grid_search(algorithm, search_mode='random'):\n",
    "    # Create the parameter grid based on the results of random search\n",
    "    if algorithm == 'nn':\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(60, 60), (60, 60, 60),\n",
    "                                   (40, 40, 20),\n",
    "                                   (60, 40, 60)],\n",
    "            'solver': ['adam'],\n",
    "            'alpha': [1e-3, 1e-4, 1e-5],\n",
    "            'batch_size': [50, 100, 250],\n",
    "            'learning_rate': ['adaptive', 'constant'],\n",
    "            'max_iter': [10000]\n",
    "        }\n",
    "        # Create a based model\n",
    "        model = MLPRegressor()\n",
    "    elif algorithm == 'rf':\n",
    "        param_grid = {\n",
    "            'random_state': [22],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [20, 50, 70, 100],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        model = RandomForestRegressor()\n",
    "    else:\n",
    "        raise Exception('Model not known')\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    if search_mode == 'grid':\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model, param_grid=param_grid,\n",
    "            cv=3, n_jobs=-1, pre_dispatch='2*n_jobs',\n",
    "            verbose=3,\n",
    "        )\n",
    "    elif search_mode == 'random':\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            estimator=model, param_distributions=param_grid,\n",
    "            cv=3, n_jobs=-1, pre_dispatch='2*n_jobs',\n",
    "            verbose=3,\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('What?')\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "model = 'rf'\n",
    "search_mode = 'grid'\n",
    "\n",
    "best_params = do_grid_search(model, search_mode=search_mode)\n",
    "# if model == 'nn':\n",
    "#     best_params = {'solver': 'adam', 'max_iter': 10000, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (40, 40, 20), 'batch_size': 50, 'alpha': 0.001}\n",
    "# elif model == 'rf':\n",
    "#     best_params = {'random_state': 22, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 20, 'bootstrap': False}\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'nn':\n",
    "    cl_best = MLPRegressor(**best_params)\n",
    "elif model == 'rf':\n",
    "    cl_best = RandomForestRegressor(**best_params)\n",
    "\n",
    "cl_best.fit(x_train, y_train)\n",
    "test_score = cl_best.score(x_test, y_test)\n",
    "train_score = cl_best.score(x_train, y_train)\n",
    "print(f'Score\\n\\nTrain: {train_score:0.3f}\\nTest: {test_score:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test\n",
    "pred_test = cl_best.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Save the regressor\n",
    "save_dir = '/home/alberto/almacen/PAUS_data/ML_z_reg'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "with open(f'{save_dir}/z_fit_NN_reg.sav', 'wb') as file:\n",
    "    pickle.dump(cl_best, file)\n",
    "# with open(f'{save_dir}/source_scaler_z_fit.sav', 'wb') as file:\n",
    "#     pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmag_train, rmag_test =\\\n",
    "    model_selection.train_test_split(rmag, test_size=0.2, random_state=split_seed)\n",
    "zspec_train, zspec_test =\\\n",
    "    model_selection.train_test_split(zspec, test_size=0.2, random_state=split_seed)\n",
    "L_Arr_train, L_Arr_test =\\\n",
    "    model_selection.train_test_split(L_Arr, test_size=0.2, random_state=split_seed)\n",
    "zphot_train, zphot_test =\\\n",
    "    model_selection.train_test_split(zphot, test_size=0.2, random_state=split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_test, zphot_test - y_test, s=0.1)\n",
    "ax.scatter(y_test, pred_test - y_test, s=0.1)\n",
    "ax.axhline(0, c='k')\n",
    "\n",
    "print(np.mean(pred_test - y_test))\n",
    "print(np.std(pred_test - y_test))\n",
    "print()\n",
    "print(np.mean(zphot_test - y_test))\n",
    "print(np.std(zphot_test - y_test))\n",
    "\n",
    "plt.ylim(-0.2, 0.2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
