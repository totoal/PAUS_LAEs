{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from paus_utils import w_central, z_NB\n",
    "\n",
    "from jpasLAEs.utils import flux_to_mag, bin_centers\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from load_paus_mocks import load_mock_dict\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_this():\n",
    "    source_cats_dir = '/home/alberto/almacen/Source_cats'\n",
    "    mock_SFG_path = f'{source_cats_dir}/LAE_12.5deg_z2.55-5_PAUS_0'\n",
    "    mock_QSO_cont_path = f'{source_cats_dir}/QSO_PAUS_contaminants_2'\n",
    "    mock_QSO_LAEs_loL_path = f'{source_cats_dir}/QSO_PAUS_LAES_2'\n",
    "    mock_QSO_LAEs_hiL_path = f'{source_cats_dir}/QSO_PAUS_LAES_hiL_2'\n",
    "    mock_GAL_path = '/home/alberto/almacen/PAUS_data/catalogs/LightCone_mock.fits'\n",
    "    mock_dict = load_mock_dict(mock_SFG_path, mock_QSO_cont_path,\n",
    "                                    mock_QSO_LAEs_loL_path, mock_QSO_LAEs_hiL_path,\n",
    "                                    mock_GAL_path, gal_fraction=1)\n",
    "\n",
    "    from load_paus_mocks import add_errors\n",
    "\n",
    "    for _, cat in mock_dict.items():\n",
    "        field_name = 'W3'\n",
    "        cat['flx'], cat['err'] = add_errors(cat['flx_0'], field_name, True)\n",
    "\n",
    "    mock_dict['GAL'].keys()\n",
    "\n",
    "    for _, cat in mock_dict.items():\n",
    "        stack_nb_ids = np.arange(12, 16 + 1)\n",
    "        synth_BB_flx = np.average(cat['flx'][stack_nb_ids],\n",
    "                                weights=cat['err'][stack_nb_ids] ** -2,\n",
    "                                axis=0)\n",
    "        cat['r_mag'] = flux_to_mag(synth_BB_flx, w_central[-4])\n",
    "\n",
    "    # Get the minimum number of candidates to set the set length\n",
    "    N_candidates_list = []\n",
    "    for mock_name, mock in mock_dict.items():\n",
    "        N_candidates_list.append(sum(np.isfinite(mock['r_mag'])))\n",
    "\n",
    "    # set_len = np.min(N_candidates_list) // 10\n",
    "    set_len = 10_000\n",
    "    print(N_candidates_list)\n",
    "    print(f'{set_len=}')\n",
    "\n",
    "    # Make the set for each class\n",
    "    tt_set = None\n",
    "    labels = None\n",
    "    rmag = None\n",
    "    zspec = None\n",
    "\n",
    "    for mock_name, mock in mock_dict.items():\n",
    "        mock_len = len(mock['zspec'])\n",
    "        if mock_name == 'SFG':\n",
    "            print('SFG')\n",
    "            nice_lya = (mock['zspec'] < 3.5) & np.isfinite(mock['r_mag'])\n",
    "        else:\n",
    "            nice_lya = np.isfinite(mock['r_mag'])\n",
    "        np.random.seed(299792458)\n",
    "        selection = np.random.choice(np.arange(mock_len)[nice_lya], set_len,\n",
    "                                    replace=False)\n",
    "        this_set = np.hstack([\n",
    "            # mock['flx'][:40, selection].T * 1e17, # NBs\n",
    "            mock['r_mag'][selection].reshape(-1, 1),\n",
    "            mock['flx'][40:45, selection].T * 1e17, # BBs\n",
    "        ])\n",
    "\n",
    "        if tt_set is None:\n",
    "            tt_set = this_set\n",
    "            this_rmag = flux_to_mag(mock['flx'][-4, selection], w_central[-4])\n",
    "            rmag = this_rmag\n",
    "            zspec = mock['zspec'][selection]\n",
    "        else:\n",
    "            tt_set = np.vstack([tt_set, this_set])\n",
    "\n",
    "            this_rmag = flux_to_mag(mock['flx'][-4, selection], w_central[-4])\n",
    "            rmag = np.concatenate([rmag, this_rmag])\n",
    "            zspec = np.concatenate([zspec, mock['zspec'][selection]])\n",
    "        \n",
    "\n",
    "    label_names = []\n",
    "    labels_list = [1, 2, 3, 3, 4]\n",
    "    for j in range(len(mock_dict)):\n",
    "        i = labels_list[j]\n",
    "        mock_name = list(mock_dict.keys())[j]\n",
    "        print(f'{i} for {mock_name}')\n",
    "        if labels is None:\n",
    "            labels = np.ones(set_len).astype(int) * i\n",
    "        else:\n",
    "            labels = np.concatenate([labels, np.ones(set_len).astype(int) * i])\n",
    "        label_names.append(mock_name)\n",
    "    label_names.append('?')\n",
    "    \n",
    "    return tt_set, rmag, zspec, label_names, labels\n",
    "\n",
    "tt_set, rmag, zspec, label_names, labels = do_this()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_seed = 299792458\n",
    "x_train, x_test, y_train, y_test =\\\n",
    "    model_selection.train_test_split(tt_set, labels, test_size=0.2,\n",
    "                                     random_state=split_seed)\n",
    "\n",
    "## Pre-processing ##\n",
    "# x_train[:, :40] /= np.sum(x_train[:, :40], axis=1).reshape(-1, 1)\n",
    "# x_train[:, 41:] /= np.sum(x_train[:, 41:], axis=1).reshape(-1, 1)\n",
    "\n",
    "# x_test[:, :40] /= np.sum(x_test[:, :40], axis=1).reshape(-1, 1)\n",
    "# x_test[:, 41:] /= np.sum(x_test[:, 41:], axis=1).reshape(-1, 1)\n",
    "x_train[:, 1:] /= np.sum(x_train[:, 1:], axis=1).reshape(-1, 1)\n",
    "x_test[:, 1:] /= np.sum(x_test[:, 1:], axis=1).reshape(-1, 1)\n",
    "\n",
    "x_train[:, 0] /= 30\n",
    "x_test[:, 0] /= 30\n",
    "\n",
    "# PCA\n",
    "# pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "# pca.fit(x_train)\n",
    "# x_train = pca.transform(x_train)\n",
    "# x_test = pca.transform(x_test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grid_search(algorithm, search_mode='random'):\n",
    "    # Create the parameter grid based on the results of random search\n",
    "    if algorithm == 'nn':\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(60, 60), (60, 60, 60), (60, 40), (50, 50),\n",
    "                                   (50, 50, 20), (60, 60, 30)],\n",
    "            'solver': ['adam'],\n",
    "            'alpha': [1e-2, 1e-3, 1e-4],\n",
    "            'batch_size': [50, 100, 250, 300],\n",
    "            'learning_rate': ['adaptive', 'constant'],\n",
    "            'max_iter': [10000]\n",
    "        }\n",
    "        # Create a based model\n",
    "        model = MLPClassifier()\n",
    "    elif algorithm == 'rf':\n",
    "        param_grid = {\n",
    "            'random_state': [22],\n",
    "            'n_estimators': [50, 100, 300, 500, 1000],\n",
    "            'bootstrap': [True, False],\n",
    "            'max_depth': [20, 50, 70, 100],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        raise Exception('Model not known')\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    if search_mode == 'grid':\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model, param_grid=param_grid,\n",
    "            cv=3, n_jobs=-1, pre_dispatch='2*n_jobs',\n",
    "            verbose=3,\n",
    "        )\n",
    "    elif search_mode == 'random':\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            estimator=model, param_distributions=param_grid,\n",
    "            cv=3, n_jobs=-1, pre_dispatch='2*n_jobs',\n",
    "            verbose=3,\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('What?')\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "model = 'nn'\n",
    "search_mode = 'random'\n",
    "\n",
    "# best_params = do_grid_search(model, search_mode=search_mode)\n",
    "if model == 'nn':\n",
    "    best_params = {'solver': 'adam', 'max_iter': 10000, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (60, 60, 60), 'batch_size': 250, 'alpha': 0.001}\n",
    "elif model == 'rf':\n",
    "    best_params = {'random_state': 22, 'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 20, 'bootstrap': False}\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'nn':\n",
    "    cl_best = MLPClassifier(**best_params)\n",
    "elif model == 'rf':\n",
    "    cl_best = RandomForestClassifier(**best_params)\n",
    "\n",
    "cl_best.fit(x_train, y_train)\n",
    "test_score = cl_best.score(x_test, y_test)\n",
    "train_score = cl_best.score(x_train, y_train)\n",
    "print(f'Score\\n\\nTrain: {train_score:0.3f}\\nTest: {test_score:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test\n",
    "pred_test = cl_best.predict(x_test)\n",
    "log_p = cl_best.predict_log_proba(x_test)\n",
    "\n",
    "# for src in range(len(pred_test)):\n",
    "#     if pred_test[src] == 4:\n",
    "#         pred_i = 2\n",
    "#     else:\n",
    "#         pred_i = pred_test[src] - 1\n",
    "#     class_log_p = log_p[src, pred_i]\n",
    "#     if class_log_p < np.log(0.88):\n",
    "#         pred_test[src] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classifier\n",
    "save_dir = '/home/alberto/almacen/PAUS_data/ML_classifier'\n",
    "with open(f'{save_dir}/source_classifier_post.sav', 'wb') as file:\n",
    "    pickle.dump(cl_best, file)\n",
    "# with open(f'{save_dir}/source_pca_post.sav', 'wb') as file:\n",
    "#     pickle.dump(pca, file)\n",
    "# with open(f'{save_dir}/source_scaler_post.sav', 'wb') as file:\n",
    "#     pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmag_train, rmag_test =\\\n",
    "    model_selection.train_test_split(rmag, test_size=0.2, random_state=split_seed)\n",
    "zspec_train, zspec_test =\\\n",
    "    model_selection.train_test_split(zspec, test_size=0.2, random_state=split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "r_mask = (rmag_test >= 2)\n",
    "cm = confusion_matrix(y_test[r_mask], pred_test[r_mask])\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "label_names_cm = ['SFG', 'QSO_cont', 'QSO_LAE', 'GAL']\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.2f',\n",
    "            xticklabels=label_names_cm, yticklabels=label_names_cm,\n",
    "            cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title('r $\\geq$ 0')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "r_mask = (rmag_test < 21)\n",
    "cm = confusion_matrix(y_test[r_mask], pred_test[r_mask])\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "label_names_cm = ['SFG', 'QSO_cont', 'QSO_LAE', 'GAL']\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.2f',\n",
    "            xticklabels=label_names_cm, yticklabels=label_names_cm,\n",
    "            cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title('r $\\geq$ 0')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# TRUE MEANS CONTAMINANT HERE\n",
    "\n",
    "y_binary = np.zeros_like(y_test).astype(bool)\n",
    "y_binary[y_test == 1] = True\n",
    "\n",
    "sfg_p = np.exp(log_p[:, 0])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_binary, sfg_p)\n",
    "\n",
    "# Compute nice threshold\n",
    "fpr_thresh_Arr = np.array([0.1, 0.05, 0.01])\n",
    "thresh_Arr = np.empty_like(fpr_thresh_Arr)\n",
    "for i, this_fpr in enumerate(fpr_thresh_Arr):\n",
    "    thresh_Arr[i] = thresholds[fpr >= this_fpr][0]\n",
    "\n",
    "print(thresh_Arr)\n",
    "\n",
    "# Represent the ROC curve\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.plot(fpr, tpr, lw=2)\n",
    "for thr in thresh_Arr:\n",
    "    # ax.axvline(fpr[thresholds == thr], ls='--', c='k')\n",
    "    ax.axvline(fpr[thresholds == thr], ls='--', c='k')\n",
    "\n",
    "ax.set_xlabel('FALSE POSITIVE RATE')\n",
    "ax.set_ylabel('TRUE POSITIVE RATE')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
