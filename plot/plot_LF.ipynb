{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import reference_LFs as refLF\n",
    "from paus_utils import *\n",
    "\n",
    "from jpasLAEs.utils import bin_centers\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_LF(region_list, NB_list):\n",
    "    this_hist = None\n",
    "    eff_vol = 0\n",
    "    for region_name in region_list:\n",
    "        for [nb1, nb2] in NB_list:\n",
    "            LF_name = f'Lya_LF_nb{nb1}-{nb2}_{region_name}'\n",
    "            pathname = f'/home/alberto/almacen/PAUS_data/Lya_LFs/{LF_name}'\n",
    "            filename_hist = f'{pathname}/hist_i_mat_{region_name}.npy'\n",
    "\n",
    "            # TODO: PROVISIONAL. As long as we are testing the mocks, weight\n",
    "            # the contributions by the mock area\n",
    "            area_dict = {\n",
    "                'SFG': 400,\n",
    "                'QSO_cont': 200,\n",
    "                'QSO_LAEs_loL': 400,\n",
    "                'QSO_LAEs_hiL': 4000,\n",
    "                'GAL': 59.97\n",
    "            }\n",
    "            this_scale_factor = area_dict['QSO_LAEs_loL'] / area_dict[region_name]\n",
    "            hist_i_mat = np.load(filename_hist) * this_scale_factor\n",
    "\n",
    "            if this_hist is None:\n",
    "                this_hist = hist_i_mat\n",
    "            else:\n",
    "                this_hist += hist_i_mat\n",
    "\n",
    "    # TODO: add one indent to the next line, and remove the for loop\n",
    "    for [nb1, nb2] in NB_list:\n",
    "        eff_vol += Lya_effective_volume(nb1, nb2, 'QSO_LAEs_loL') # TODO: Change the area with region_name\n",
    "\n",
    "    L_bins = np.load(f'{pathname}/LF_L_bins.npy')\n",
    "    bin_width = [L_bins[i + 1] - L_bins[i] for i in range(len(L_bins) - 1)]\n",
    "\n",
    "    L_LF_err_percentiles = np.percentile(this_hist, [16, 50, 84], axis=0)\n",
    "    hist_median = L_LF_err_percentiles[1]\n",
    "\n",
    "    boots_path = f'/home/alberto/almacen/PAUS_data/Lya_LFs/bootstrap_errors'\n",
    "    yerr_minus = np.load(f'{boots_path}/LF_err_minus_nb{nb1}-{nb2}.npy')\n",
    "    yerr_plus = np.load(f'{boots_path}/LF_err_plus_nb{nb1}-{nb2}.npy')\n",
    "\n",
    "    this_LF = hist_median / bin_width / eff_vol\n",
    "    LF_boots = np.load(f'{boots_path}/median_LF_nb{nb1}-{nb2}.npy')\n",
    "    # Fix yerr_minus when LF_boots == 0\n",
    "    yerr_minus[LF_boots == 0] = this_LF[LF_boots == 0]\n",
    "\n",
    "    this_LF_dict = {\n",
    "        'LF_bins': bin_centers(L_bins),\n",
    "        'LF_total': this_LF,\n",
    "        'LF_total_err': [yerr_minus, yerr_plus],\n",
    "    }\n",
    "\n",
    "    return this_LF_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the QSO mock to compare\n",
    "from load_paus_mocks import load_qso_mock\n",
    "from jpasLAEs.utils import flux_to_mag\n",
    "\n",
    "source_cats_dir = '/home/alberto/almacen/Source_cats'\n",
    "mock_path = f'{source_cats_dir}/QSO_PAUS_LAES_2'\n",
    "mock = load_qso_mock(mock_path)\n",
    "\n",
    "mock['r_mag'] = flux_to_mag(mock['flx_0'][-4], w_central[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_list = ['SFG', 'QSO_cont', 'QSO_LAEs_loL',\n",
    "               'GAL']\n",
    "nb_list = [[0, 2], [2, 4], [4, 6], [6, 8],\n",
    "              [8, 10], [10, 12], [12, 14], [14, 16]]\n",
    "\n",
    "LyaLF = load_combined_LF(survey_list, nb_list)\n",
    "\n",
    "# LF of the mock\n",
    "nb1, nb2 = nb_list[0][0], nb_list[-1][-1]\n",
    "L_bins = np.linspace(40, 47, 100)\n",
    "L_bins_c = bin_centers(L_bins)\n",
    "L_bins_w = L_bins[1] - L_bins[0]\n",
    "mask_mock = ((NB_z(mock['zspec']) >= nb1)\n",
    "             & (NB_z(mock['zspec']) <= nb2)\n",
    "             & (mock['EW0_lya_spec'] > 30)\n",
    "             & (mock['r_mag'] < 24)\n",
    "             & (mock['r_mag'] > 17))\n",
    "mock_LF = np.histogram(mock['L_lya_spec'][mask_mock], L_bins)[0]\\\n",
    "              / L_bins_w / Lya_effective_volume(nb1, nb2, 400)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.errorbar(LyaLF['LF_bins'], LyaLF['LF_total'],\n",
    "            yerr=LyaLF['LF_total_err'],\n",
    "            fmt='s', ls='', mfc='limegreen', mec='k',\n",
    "            ms=7, ecolor='k', capsize=3)\n",
    "\n",
    "ax.plot(L_bins_c, mock_LF)\n",
    "\n",
    "ax.set(yscale='log',\n",
    "       xlim=(42.5, 46), ylim=(1e-8, 1e-4))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
